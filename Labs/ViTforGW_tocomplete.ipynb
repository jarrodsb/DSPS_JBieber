{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarrodsb/DSPS_JBieber/blob/main/Labs/ViTforGW_tocomplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import pandas as pd\n",
        "from skimage import io\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import rescale\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "import pylab as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "DIRECTLYFROMGRAVITYSPY = False"
      ],
      "metadata": {
        "id": "FSatv31GbrIT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "an-8nvV2bmRn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def read_and_crop_image(filename, x, y):\n",
        "    \"\"\"Read in a crop part of image you want to keep\n",
        "\n",
        "    Parameters\n",
        "        filename (str):\n",
        "            the file you would like to pixelize\n",
        "\n",
        "        x (float, list):\n",
        "            xrange of pixels to keep\n",
        "\n",
        "        y (float, list):\n",
        "            yrange of pixels to keep\n",
        "\n",
        "\n",
        "    Returns\n",
        "        image_data (`np.array):\n",
        "            this images is taken from rgb to gray scale\n",
        "            and then downsampled by the resolution.\n",
        "    \"\"\"\n",
        "    xmin = x[0]\n",
        "    xmax = x[1]\n",
        "    ymin = y[0]\n",
        "    ymax = y[1]\n",
        "    image_data = io.imread(filename)\n",
        "    image_data = image_data[xmin:xmax, ymin:ymax, :3]\n",
        "    return image_data\n",
        "\n",
        "\n",
        "def read_grayscale(filename, resolution=0.3, x=[66, 532], y=[105, 671],\n",
        "                   verbose=False):\n",
        "    \"\"\"Convert image from RGB to Gray, downsample\n",
        "\n",
        "    Parameters\n",
        "        filename (str):\n",
        "            the file you would like to pixelize\n",
        "\n",
        "        resolution (float, optional):\n",
        "            default: 0.3\n",
        "\n",
        "        verbose (bool, optional):\n",
        "            default: False\n",
        "\n",
        "    Returns\n",
        "        image_data (`np.array):\n",
        "            this images is taken from rgb to gray scale\n",
        "            and then downsampled by the resolution.\n",
        "    \"\"\"\n",
        "    image_data = read_and_crop_image(filename, x=x, y=y)\n",
        "\n",
        "    image_data = rgb2gray(image_data)\n",
        "    image_data = rescale(image_data, resolution, mode='constant',\n",
        "                         preserve_range='True', channel_axis=None)\n",
        "\n",
        "    #dim = int(reduce(lambda x, y: x * y, image_data.shape))\n",
        "    #image_data = np.reshape(image_data, (dim))\n",
        "    image_data = np.array(image_data, dtype='f')\n",
        "\n",
        "    return image_data\n",
        "\n",
        "\n",
        "def read_rgb(filename, resolution=0.3, x=[66, 532], y=[105, 671],\n",
        "             verbose=False):\n",
        "    \"\"\"Convert image from RGB to Gray, downsample\n",
        "\n",
        "    Parameters\n",
        "        filename (str):\n",
        "            the file you would like to pixelize\n",
        "\n",
        "        resolution (float, optional):\n",
        "            default: 0.3\n",
        "\n",
        "        verbose (bool, optional):\n",
        "            default: False\n",
        "\n",
        "    Returns\n",
        "        image_data (`np.array):\n",
        "            this images is taken from rgb to gray scale\n",
        "            and then downsampled by the resolution.\n",
        "    \"\"\"\n",
        "    image_data = read_and_crop_image(filename, x=x, y=y)\n",
        "    image_data = rescale(image_data, resolution, mode='constant',\n",
        "                         preserve_range='True', channel_axis=-1)\n",
        "    dim = int(reduce(lambda x, y: x * y, image_data[:, :, 0].shape))\n",
        "    image_data_r = np.reshape(image_data[:, :, 0], (dim))\n",
        "    image_data_g = np.reshape(image_data[:, :, 1], (dim))\n",
        "    image_data_b = np.reshape(image_data[:, :, 2], (dim))\n",
        "\n",
        "    return image_data_r, image_data_g, image_data_b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the cell below to select your classes and read in teh images directly from GravitySpy"
      ],
      "metadata": {
        "id": "mGnLhVxAlXQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if DIRECTLYFROMGRAVITYSPY:\n",
        "  labels = []\n",
        "  imgs = np.zeros((2264, 140, 170))\n",
        "  j, k = 0, 0\n",
        "  for i,f in enumerate(glob(\"H1L1/Chirp/*\")):\n",
        "      #print(f)\n",
        "      if k == 264:\n",
        "          break\n",
        "      img = read_grayscale(f)\n",
        "      if i%100 == 0 :\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"Chirp {i} \")\n",
        "          plt.show()\n",
        "      imgs[j] = img\n",
        "      labels.append([\"chirp\"])\n",
        "      k += 1\n",
        "      j += 1\n",
        "  print(i)\n",
        "  k = 0\n",
        "  for i,f in enumerate(glob(\"H1L1/Low_Frequency_Burst/*\")) :\n",
        "      if k == 1000:\n",
        "          break\n",
        "      img = read_grayscale(f)\n",
        "      if i%100 == 0 :\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"Low_Frequency_Burst {i}\")\n",
        "          plt.show()\n",
        "\n",
        "      imgs[j] = img\n",
        "      labels.append([\"low_frequency_burst\"])\n",
        "      k += 1\n",
        "      j += 1\n",
        "  print(i,j,k)\n",
        "\n",
        "  k = 0\n",
        "  for i,f in enumerate(glob(\"H1L1/Koi_Fish/*\")) :\n",
        "      if k == 1000:\n",
        "          break\n",
        "      img = read_grayscale(f)\n",
        "      if i%100 == 0 :\n",
        "          plt.imshow(img)\n",
        "          plt.title(f\"koi_fish {i}\")\n",
        "          plt.show()\n",
        "\n",
        "      imgs[j] = img\n",
        "      labels.append([\"koi_fish\"])\n",
        "      k += 1\n",
        "      j += 1\n"
      ],
      "metadata": {
        "id": "ALKdI5yAbnql"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run the two cells below to read the pre-made array of data"
      ],
      "metadata": {
        "id": "pYgRa--LlhDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not DIRECTLYFROMGRAVITYSPY:\n",
        "  labels = pd.read_csv(\"gw_labels.csv\", index_col=0)\n",
        "  labels"
      ],
      "metadata": {
        "id": "4UxyUN4UccgR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not DIRECTLYFROMGRAVITYSPY:\n",
        "  imgs = np.load(open(\"gw_imgs.npy\", \"rb\"))\n",
        "  imgs.shape\n",
        "#reshaping images with a new axis because vision transformers are commonly built for 3 axis images\n",
        "imgs = imgs[:,:,:,np.newaxis]\n",
        "imgs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "S2DYh662kMP_",
        "outputId": "b5bd897f-1a8d-4e86-963a-e052ed0860ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 41549808 into shape (2264,140,170)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2956252125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mDIRECTLYFROMGRAVITYSPY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gw_imgs.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#reshaping images with a new axis because vision transformers are commonly built for 3 axis images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    482\u001b[0m                                           max_header_size=max_header_size)\n\u001b[1;32m    483\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    485\u001b[0m                                          \u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                                          max_header_size=max_header_size)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 41549808 into shape (2264,140,170)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#show a few images\n",
        "np.random.seed(302)\n",
        "for i in np.random.choice(imgs.shape[0], 10):\n",
        "  plt.imshow(imgs[i])\n",
        "  plt.title(labels.iloc[i])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "avFLhJeullpi",
        "outputId": "da0ef946-3c03-4933-e35a-d4de377bffab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'imgs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2717412707.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#show a few images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m302\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'imgs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhWN56lxoKj9",
        "outputId": "cdc1c221-2eeb-47db-ca29-b8b631b55bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['chirp', 'low_frequency_burst', 'koi_fish'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Encode string labels into numerical format\n",
        "#label_encoder = LabelEncoder()\n",
        "#encoded_labels_sparse = label_encoder.fit_transform(labels)\n",
        "#sparse encoder makes a 1D label with numbers 0-n, use sparse_categorical_crossentropy loss\n",
        "\n",
        "# Convert to one-hot encoded format\n",
        "num_classes = len(labels[\"0\"].unique())\n",
        "encoded_labels = tf.keras.utils.to_categorical(encoded_labels_sparse, num_classes=num_classes)\n",
        "\n",
        "encoded_labels[:20], encoded_labels[-20:]\n",
        "#use categorical_crossentropy loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0d64nhneJte",
        "outputId": "2709fc29-6e4e-4fd0-a645-1f837b87ef4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.]]),\n",
              " array([[0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 1., 0.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTANT:\n",
        "# stratify: Perform the train-test split with same fraction of labels in training and testing - for imbalanced datasets\n",
        "# Shuffle the data because the labels are ordered!\n",
        "X_train, X_test, y_train, y_test = train_test_split(imgs, encoded_labels, test_size=0.1, random_state=302, shuffle=True, stratify=encoded_labels)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Show the mapping of original labels to encoded numbers\n",
        "print(\"\\nLabel encoding mapping:\")\n",
        "for i, label_name in enumerate(label_encoder.classes_):\n",
        "    print(f\"{label_name}: {i}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10U4xNYrhTuf",
        "outputId": "750f2b3e-52ba-4c17-a7be-0225517a43f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2037, 140, 170, 1)\n",
            "X_test shape: (227, 140, 170, 1)\n",
            "y_train shape: (2037, 3)\n",
            "y_test shape: (227, 3)\n",
            "\n",
            "Label encoding mapping:\n",
            "chirp: 0\n",
            "koi_fish: 1\n",
            "low_frequency_burst: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "class PatchExtract(layers.Layer):\n",
        "    \"\"\"Extract patches from images.\"\"\"\n",
        "    def __init__(self, patch_size):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding='VALID'\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches\n",
        "\n",
        "class MinimalViT(keras.Model):\n",
        "    \"\"\"Minimal Vision Transformer for 140x170 images.\"\"\"\n",
        "    def __init__(self, image_size=(140, 170), patch_size=14, num_classes=3):\n",
        "        super().__init__()\n",
        "        # Adjust to be divisible by patch_size\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size)\n",
        "        self.embed_dim = 64\n",
        "\n",
        "        # Patch extraction and embedding\n",
        "        self.patch_extract = PatchExtract(patch_size)\n",
        "        self.patch_embed = layers.Dense(self.embed_dim)\n",
        "\n",
        "        # CLS token and position embeddings\n",
        "        self.cls_token = self.add_weight(\n",
        "            shape=(1, 1, self.embed_dim),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='cls_token'\n",
        "        )\n",
        "        self.pos_embed = self.add_weight(\n",
        "            shape=(1, self.num_patches + 1, self.embed_dim),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='pos_embed'\n",
        "        )\n",
        "\n",
        "        # Transformer encoder layers (corrected structure)\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6) self.attn = layers.MultiHeadAttention(num_heads=4, key_dim=self.embed_dim // 4, dropout=0.1) self.dropout_attn = layers.Dropout(0.1)\n",
        "\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6) self.mlp = keras.Sequential([ layers.Dense(self.embed_dim * 2, activation='gelu'), layers.Dropout(0.1), layers.Dense(self.embed_dim), layers.Dropout(0.1), ])\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "        # Extract patches\n",
        "        patches = self.patch_extract(inputs) # (batch, num_patches, patch_sizepatch_size3)\n",
        "\n",
        "        # Embed patches\n",
        "        x = self.patch_embed(patches) # (batch, num_patches, embed_dim)\n",
        "\n",
        "\n",
        "        # Add CLS token\n",
        "        cls_tokens = tf.tile(self.cls_token, [batch_size, 1, 1])\n",
        "        x = tf.concat([cls_tokens, x], axis=1)  # (batch, num_patches+1, embed_dim)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Single transformer block logic\n",
        "        # Layer Normalization 1\n",
        "        x_norm1 = self.norm1(x)\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        attn_output = self.attn(query=x_norm1, key=x_norm1, value=x_norm1,\n",
        "                                training=training)\n",
        "        attn_output = self.dropout_attn(attn_output,\n",
        "                                                  training=training)\n",
        "        x = x + attn_output # Add skip connection\n",
        "\n",
        "        # Layer Normalization 2 and MLP\n",
        "        x_norm2 = self.norm2(x)\n",
        "        mlp_output = self.mlp(x_norm2, training=training)\n",
        "        x = x + mlp_output # Add skip connection\n",
        "\n",
        "        # Use CLS token for classification\n",
        "        x = self.norm(x[:, 0]) # take CLS toekn\n",
        "\n",
        "        return self.head(x)\n"
      ],
      "metadata": {
        "id": "ZohJmMqzcalZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "num_classes = 3\n",
        "# Assuming your data is already loaded\n",
        "# X_train shape: (n_samples, 140, 170, 3) - now correct with 3 channels\n",
        "# y_train shape: (n_samples, 3) - one-hot encoded\n",
        "\n",
        "# 1. Create minimal ViT model\n",
        "# IMPORTANT: Set num_classes to 3, matching your dataset's actual number of classes\n",
        "model = MinimalViT(patch_size=14, num_classes=num_classes) # Use the actual\n",
        "\n",
        "# 2. Build with input shape\n",
        "# Input shape should now be (None, 140, 170, 3) as imgs_processed will have 3 channels\n",
        "model.build(input_shape=(None, 140, 170, 1))\n",
        "\n",
        "# 3. Compile\n",
        "# IMPORTANT: Change loss to 'categorical_crossentropy' because y_train is one-hot encoded\n",
        "model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'] )\n",
        "\n",
        "# 4. Train\n",
        "history = model.fit( X_train, y_train, validation_split=0.2, epochs=10, batch_size=32 )\n",
        "\n",
        "# 5. Evaluate\n",
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {acc:.2%}\")\n",
        "\n",
        "# 6. Predict\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKuWxOARcyJM",
        "outputId": "72904d0a-843d-4212-bbdd-de424d337347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'minimal_vi_t_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 206ms/step - accuracy: 0.4042 - loss: 1.2895 - val_accuracy: 0.8137 - val_loss: 0.8375\n",
            "Epoch 2/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 156ms/step - accuracy: 0.7903 - loss: 0.6087 - val_accuracy: 0.8971 - val_loss: 0.1959\n",
            "Epoch 3/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 167ms/step - accuracy: 0.9546 - loss: 0.1312 - val_accuracy: 0.9926 - val_loss: 0.0210\n",
            "Epoch 4/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 166ms/step - accuracy: 0.9857 - loss: 0.0406 - val_accuracy: 0.9877 - val_loss: 0.0339\n",
            "Epoch 5/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 142ms/step - accuracy: 0.9889 - loss: 0.0381 - val_accuracy: 0.9877 - val_loss: 0.0154\n",
            "Epoch 6/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9962 - loss: 0.0135 - val_accuracy: 0.9975 - val_loss: 0.0099\n",
            "Epoch 7/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 164ms/step - accuracy: 0.9944 - loss: 0.0237 - val_accuracy: 0.9828 - val_loss: 0.0348\n",
            "Epoch 8/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.9959 - loss: 0.0107 - val_accuracy: 0.9951 - val_loss: 0.0084\n",
            "Epoch 9/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 0.9975 - val_loss: 0.0164\n",
            "Epoch 10/10\n",
            "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 156ms/step - accuracy: 0.9979 - loss: 0.0080 - val_accuracy: 0.9975 - val_loss: 0.0314\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9979 - loss: 0.0107\n",
            "Test accuracy: 99.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 17 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ae0dd61e3e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 82ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])"
      ],
      "metadata": {
        "id": "qXUYJ3GejRZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1980cfb1"
      },
      "source": [
        "# Convert model predictions from probabilities to class labels\n",
        "y_pred_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Convert one-hot encoded y_test back to original class labels for comparison\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get class names from the label encoder for better readability in the matrix\n",
        "class_names = label_encoder.classes_\n",
        "\n",
        "print(\"Predicted classes sample:\", y_pred_classes[:10])\n",
        "print(\"True classes sample:\", y_true_classes[:10])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fde24c43"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "usK-m-J2j9Na"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}